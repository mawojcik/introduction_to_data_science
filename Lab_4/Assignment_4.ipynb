{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Today we are going to perform the simple classification of the amazon reviews' sentiment.\n",
    "\n",
    "### Please, download the dataset amazon_baby.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:50:18.903508Z",
     "start_time": "2023-12-18T21:50:17.837494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                name  \\\n0                           Planetwise Flannel Wipes   \n1                              Planetwise Wipe Pouch   \n2                Annas Dream Full Quilt with 2 Shams   \n3  Stop Pacifier Sucking without tears with Thumb...   \n4  Stop Pacifier Sucking without tears with Thumb...   \n\n                                              review  rating  \n0  These flannel wipes are OK, but in my opinion ...       3  \n1  it came early and was not disappointed. i love...       5  \n2  Very soft and comfortable and warmer than it l...       5  \n3  This is a product well worth the purchase.  I ...       5  \n4  All of my kids have cried non-stop when I trie...       5  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>review</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Planetwise Flannel Wipes</td>\n      <td>These flannel wipes are OK, but in my opinion ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Planetwise Wipe Pouch</td>\n      <td>it came early and was not disappointed. i love...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Annas Dream Full Quilt with 2 Shams</td>\n      <td>Very soft and comfortable and warmer than it l...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n      <td>This is a product well worth the purchase.  I ...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n      <td>All of my kids have cried non-stop when I trie...</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "baby_df = pd.read_csv('amazon_baby.csv')\n",
    "baby_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (data preparation)\n",
    "a) Remove punctuation from reviews using the given function.   \n",
    "b) Replace all missing (nan) revies with empty \"\" string.  \n",
    "c) Drop all the entries with rating = 3, as they have neutral sentiment.   \n",
    "d) Set all positive ($\\geq$4) ratings to 1 and negative($\\leq$2) to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:50:19.958683Z",
     "start_time": "2023-12-18T21:50:18.918141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#b)Replace all missing (nan) reviews with empty \"\" string\n",
    "\"\"\"firstly we repalce all missing reviews with empty strings,\n",
    "so we do not have any \"garbage\" data when removing punctuation\"\"\"\n",
    "\n",
    "baby_df[\"review\"] = baby_df[\"review\"].fillna(\"\")\n",
    "\n",
    "# a)Remove punctuation from reviews using the given function\n",
    "baby_df[\"review\"].apply(remove_punctuation)\n",
    "\n",
    "#short test: \n",
    "print(baby_df[\"review\"][4] == 'All of my kids have cried nonstop when I tried to ween them off their pacifier until I found Thumbuddy To Loves Binky Fairy Puppet  It is an easy way to work with your kids to allow them to understand where their pacifier is going and help them part from itThis is a must buy book and a great gift for expecting parents  You will save them soo many headachesThanks for this book  You all rock')\n",
    "print(remove_punctuation(baby_df[\"review\"][4]) == 'All of my kids have cried nonstop when I tried to ween them off their pacifier until I found Thumbuddy To Loves Binky Fairy Puppet  It is an easy way to work with your kids to allow them to understand where their pacifier is going and help them part from itThis is a must buy book and a great gift for expecting parents  You will save them soo many headachesThanks for this book  You all rock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:50:19.972670Z",
     "start_time": "2023-12-18T21:50:19.960817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b)\n",
    "\"\"\"\n",
    "implemented above\n",
    "\"\"\"\n",
    "\n",
    "#short test:\n",
    "baby_df[\"review\"][38] == baby_df[\"review\"][38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:50:20.032123Z",
     "start_time": "2023-12-18T21:50:19.972258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#c) Drop all the entries with rating = 3, as they have neutral sentiment\n",
    "baby_df = baby_df.drop(baby_df[baby_df[\"rating\"] == 3].index)\n",
    "\n",
    "#short test:\n",
    "sum(baby_df[\"rating\"] == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:50:20.059066Z",
     "start_time": "2023-12-18T21:50:20.012039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#d) Set all positive >= 4 ratings to 1 and negative <= 2 to -1\n",
    "\"\"\"\n",
    "Here we map date to either positive or negative\n",
    "This is why we had to drop \"3\". In this case it was neutral\n",
    "\"\"\"\n",
    "\n",
    "baby_df[\"rating\"] = np.where(baby_df[\"rating\"] < 3, -1, 1)\n",
    "#short test:\n",
    "sum(baby_df[\"rating\"]**2 != 1) # no element is different from 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "In order to analyze strings, we need to assign them numerical values. We will use one of the simplest string representation, which transforms strings into the $n$ dimensional vectors. The number of dimensions will be the size of our dictionary, and then the values of the vector will represent the number of appereances of the given word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:50:20.147174Z",
     "start_time": "2023-12-18T21:50:20.056083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adore' 'and' 'apples' 'bananas' 'dislike' 'hate' 'like' 'oranges' 'they'\n",
      " 'we']\n",
      "[[0 0 1 0 0 0 1 0 0 1]\n",
      " [0 0 0 0 0 1 0 1 0 1]\n",
      " [1 0 0 1 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 2 1 0 1]\n",
      " [0 0 0 1 1 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "reviews_train_example = [\"We like apples\",\n",
    "                   \"We hate oranges\",\n",
    "                   \"I adore bananas\",\n",
    "                   \"We like like apples and oranges\",\n",
    "                   \"They dislike bananas\"]\n",
    "\n",
    "X_train_example = vectorizer.fit_transform(reviews_train_example)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X_train_example.todense())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:50:20.148377Z",
     "start_time": "2023-12-18T21:50:20.064649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 0 1 0 1 0]\n",
      " [0 1 1 1 0 1 0 1 0 1]\n",
      " [0 0 0 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "reviews_test_example = [\"They like bananas\",\n",
    "                   \"We hate oranges bananas and apples\",\n",
    "                   \"We love bananas\"] #New word!\n",
    "\n",
    "X_test_example = vectorizer.transform(reviews_test_example)\n",
    "\n",
    "print(X_test_example.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should acknowledge few facts. Firstly, CountVectorizer does not take order into account. Secondly, it ignores one-letter words (this can be changed during initialization). Finally, for test values, CountVectorizer ignores words which are not in it's dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 \n",
    "a) Split dataset into training and test sets.     \n",
    "b) Transform reviews into vectors using CountVectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:50:20.168221Z",
     "start_time": "2023-12-18T21:50:20.073063Z"
    }
   },
   "outputs": [],
   "source": [
    "#a)Split dataset into training and test sets.\n",
    "train_df = baby_df.sample(frac=.8, random_state=42)\n",
    "test_df = baby_df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:50:25.698386Z",
     "start_time": "2023-12-18T21:50:20.174266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acorde' 'acordian' 'acorn' 'acorns' 'acorss' 'acosco' 'acoss'\n",
      " 'acostumbrarla' 'acound' 'acoustic' 'acoustics' 'acquaintance'\n",
      " 'acquaintances' 'acquainted' 'acquainting' 'acquard' 'acquire' 'acquired'\n",
      " 'acquiring' 'acquisition' 'acquisitions' 'acre' 'acreage' 'acres' 'acrid'\n",
      " 'acrobat' 'acrobatic' 'acrobatics' 'acrobats' 'acronym' 'acronyms'\n",
      " 'across' 'acrossed' 'acrossthe' 'acrylic' 'acrylonitrile' 'act' 'actally'\n",
      " 'actaully' 'actaulyl' 'acted' 'actident' 'actied' 'acting' 'action'\n",
      " 'actions' 'activ3' 'activate' 'activated' 'activatei' 'activates'\n",
      " 'activating' 'activation' 'activator' 'active' 'actived' 'actively'\n",
      " 'actives' 'activiation' 'activies' 'activited' 'activites' 'activities'\n",
      " 'activitites' 'activity' 'activitygym' 'actm' 'actoually' 'acts'\n",
      " 'actting' 'actual' 'actuality' 'actualize' 'actuallt' 'actually'\n",
      " 'actuallydon' 'actuallyy' 'actualy' 'actuate' 'actuated' 'actully'\n",
      " 'actuly' 'actvities' 'acual' 'acually' 'acupuncture' 'acura' 'acurate'\n",
      " 'acurately' 'acure' 'acustom' 'acustomed' 'acutal' 'acutally' 'acute'\n",
      " 'ad' 'ada' 'adage' 'adain' 'adam' 'adamant' 'adamantly' 'adament' 'adams'\n",
      " 'adapeter' 'adapor' 'adapt' 'adaptability' 'adaptable' 'adaptableyou'\n",
      " 'adaptaplug' 'adaptation' 'adaptations' 'adapte' 'adapted' 'adapter'\n",
      " 'adapterbritax' 'adapters' 'adaptersit' 'adapterswe' 'adapting'\n",
      " 'adaption' 'adaptive' 'adaptor' 'adaptorfast' 'adaptors' 'adapts'\n",
      " 'adaquate' 'adaquite' 'adavinci' 'adays' 'adc' 'add' 'addapter' 'added'\n",
      " 'addeda' 'addeded' 'addendum' 'addico' 'addict' 'addicted' 'addictedit'\n",
      " 'addicting' 'addiction' 'addictive' 'addictively' 'addicts' 'addiditon'\n",
      " 'addie' 'adding' 'addion' 'addiotional' 'addison' 'additiional'\n",
      " 'additinal' 'additinoal' 'addition' 'additional' 'additionally'\n",
      " 'additionals' 'additionalsafe' 'additionaly' 'additionfunction'\n",
      " 'additions' 'additive' 'additives' 'additonal' 'additonally' 'additude'\n",
      " 'addled' 'addn' 'addon' 'addorable' 'address' 'addresscons' 'addressed'\n",
      " 'addresses' 'addressing' 'adds' 'addt' 'addtion' 'addtional'\n",
      " 'addventurous' 'addvertisment' 'addy' 'adecuado' 'adehsive' 'adelaide'\n",
      " 'adelynn' 'adem' 'ademas' 'aden' 'adenanais' 'adept' 'adeptly' 'adequate'\n",
      " 'adequately' 'adhd' 'adherance' 'adhere' 'adhered' 'adherence' 'adheres'\n",
      " 'adhering' 'adhesion' 'adhesive' 'adhesiveness' 'adhesives' 'adhessive'\n",
      " 'adiamo' 'adidas' 'adieu' 'adin' 'adipates' 'adiri' 'adition' 'aditional'\n",
      " 'adived' 'adj' 'adjacent' 'adjective' 'adjectives' 'adjoining' 'adjsut'\n",
      " 'adjuatable' 'adjunct' 'adjuncts' 'adjusable' 'adjusment' 'adjust'\n",
      " 'adjust6' 'adjustability' 'adjustabilitybaby' 'adjustabilty' 'adjustable'\n",
      " 'adjustable5' 'adjustablehow' 'adjustablele' 'adjustableness'\n",
      " 'adjustablesecure' 'adjustablesince' 'adjustablity' 'adjustble'\n",
      " 'adjustcons' 'adjusted' 'adjustement' 'adjuster' 'adjusters' 'adjustes'\n",
      " 'adjusti' 'adjustibile' 'adjustible' 'adjusting' 'adjustmant'\n",
      " 'adjustment' 'adjustments' 'adjustmentseffortlessly' 'adjustmentthe'\n",
      " 'adjustmetnts' 'adjustmost' 'adjustmy' 'adjustor' 'adjustpower' 'adjusts'\n",
      " 'adjustt' 'adjustthe' 'adjutanle' 'adjutment' 'adler' 'admin'\n",
      " 'administer' 'administered' 'administering' 'administration'\n",
      " 'administrative' 'administrators' 'admirable' 'admirably' 'admiration'\n",
      " 'admire' 'admired' 'admiring' 'admiringly' 'admission' 'admit' 'admits'\n",
      " 'admitt' 'admittance' 'admitted' 'admittedly' 'admitting' 'admonishes'\n",
      " 'admonishment' 'adn' 'ado' 'adolescent' 'adopt' 'adopted' 'adoptee'\n",
      " 'adopting' 'adoption' 'adoptive' 'adopts' 'ador' 'adora' 'adorabe'\n",
      " 'adorable' 'adorablein' 'adorablethe' 'adorablevery' 'adorably' 'adoram'\n",
      " 'adorarable' 'adoration' 'adorbie' 'adorbs' 'adorded' 'adore' 'adoreable'\n",
      " 'adoreablely' 'adored' 'adores' 'adorible' 'adoring' 'adorn' 'adorned'\n",
      " 'adornment' 'adorou' 'adpater' 'adpostal' 'adpt' 'adquirir' 'adrenaline'\n",
      " 'adriana' 'adroable' 'ads' 'adsolutely' 'adsorbant' 'adujstable' 'adult'\n",
      " 'adulthood' 'adultlocks' 'adulto' 'adults' 'adultsmall' 'adustable'\n",
      " 'adustment' 'adv' 'advaced' 'advance' 'advanced' 'advancement'\n",
      " 'advancements' 'advances' 'advancing' 'advandage' 'advant' 'advantage'\n",
      " 'advantagemat' 'advantageous' 'advantages' 'advantageshot' 'advent'\n",
      " 'adventerous' 'adventure' 'adventureand' 'adventurefor' 'adventurer'\n",
      " 'adventures' 'adventuring' 'adventurous' 'adverb' 'adverise' 'adverse'\n",
      " 'adversely' 'adverstised' 'adverstisement' 'advert' 'advertise'\n",
      " 'advertised' 'advertised4' 'advertisedit' 'advertisedsignificant'\n",
      " 'advertisedtemperature' 'advertisement' 'advertisements' 'advertises'\n",
      " 'advertising' 'advertized' 'advertizes' 'advice' 'advices' 'advidce'\n",
      " 'advil' 'advisable' 'advise' 'advised' 'adviser' 'advises' 'advising'\n",
      " 'advisor' 'advisory' 'advocacy' 'advocate' 'advocated' 'advocates'\n",
      " 'advocating' 'ae' 'aerate' 'aerated' 'aerator' 'aereola' 'aerio' 'aero'\n",
      " 'aerobed' 'aerobics' 'aerobile' 'aerodynamic' 'aeroflow' 'aerosol'\n",
      " 'aerospace' 'aestetically' 'aestheteically' 'aesthetic' 'aesthetically'\n",
      " 'aestheticallypleasing' 'aesthetics' 'aestheticsof' 'aevenflo' 'aewsome'\n",
      " 'afaid' 'afantastic' 'afar' 'afd' 'afer' 'aferwards' 'afetr' 'affair'\n",
      " 'affairs' 'affect' 'affected' 'affecting' 'affection' 'affectionate'\n",
      " 'affectionately' 'affective' 'affects' 'affeecting' 'affiad' 'affiliate'\n",
      " 'affiliated' 'affiliates' 'affiliation' 'affinity' 'affirmation'\n",
      " 'affirmed' 'affix' 'affixed' 'affixes' 'affixing' 'affodable' 'afforable'\n",
      " 'afforadable' 'afford' 'affordability' 'affordable' 'affordablecons'\n",
      " 'affordableeasy' 'affordablei' 'affordably' 'affordale' 'afforded'\n",
      " 'affording' 'affords' 'affraid' 'affront' 'afg' 'afghanistan'\n",
      " 'aficionados' 'afield' 'afisher' 'afix' 'afixed' 'aflac' 'afloat' 'afoga'\n",
      " 'afoogo' 'aforementioned' 'afoul' 'afraid' 'afresh' 'afriad' 'africa'\n",
      " 'african' 'afriend' 'afrraid' 'afte' 'after' 'aftera' 'afterall'\n",
      " 'afterbreadingyhe' 'afterbuying' 'aftermarket' 'aftermath' 'afternoon'\n",
      " 'afternoons' 'afteroon' 'afterordering' 'aftershave' 'aftershe'\n",
      " 'aftertaste' 'afterthought' 'afterward' 'afterwards'\n",
      " 'afterwardssubstantial' 'afterwardsw' 'afterwhile' 'afterwoods'\n",
      " 'afterwords' 'afthom' 'afv' 'ag10' 'agaian' 'again' 'again2' 'again3'\n",
      " 'againand' 'againg' 'againrapture' 'agains' 'against' 'againstcons'\n",
      " 'againstthe' 'againt' 'againvery' 'agaist' 'agao' 'agape' 'age' 'aged'\n",
      " 'agei' 'ageing' 'ageless' 'agencies' 'agency' 'agenda' 'agent' 'agents'\n",
      " 'ages' 'agescons' 'ageshugs' 'agesused' 'agethe' 'agewise' 'agey' 'aggie'\n",
      " 'aggitation' 'aggravate' 'aggravated' 'aggravates' 'aggravating'\n",
      " 'aggravation' 'aggregate' 'aggregation' 'aggresive' 'aggression'\n",
      " 'aggressive' 'aggressively' 'aggrevating' 'aggrevation' 'aggrivated'\n",
      " 'aggrivating' 'agian' 'agift' 'agile' 'agility' 'agin' 'aging' 'agion'\n",
      " 'agitate' 'agitated' 'agitates' 'agitating' 'agitation' 'agitator'\n",
      " 'agnew' 'ago' 'agonized' 'agonizing' 'agony' 'agood' 'agptek' 'agraco'\n",
      " 'agradezco' 'agravated' 'agrave' 'agreat' 'agree' 'agreeable' 'agreeably'\n",
      " 'agreed' 'agreee' 'agreeing' 'agreement' 'agrees' 'agrere' 'agressive'\n",
      " 'agrivated' 'ags' 'agua' 'agujero' 'ah' 'aha' 'ahahah' 'ahahahah'\n",
      " 'ahappier' 'ahe' 'ahead' 'aheadof' 'ahem' 'ahen' 'ahh' 'ahha' 'ahhh'\n",
      " 'ahhhed' 'ahhhh' 'ahhhhh' 'ahhhhhh' 'ahhhhhhh' 'ahhhs' 'ahhing' 'ahhs'\n",
      " 'ahi' 'ahip' 'ahipping' 'ahit' 'ahnd' 'ahold' 'aholic' 'ahora'\n",
      " 'ahorramos' 'ahpolstered' 'ahs' 'ahswiwoyo' 'ahuyama' 'ahve' 'ai' 'ai1'\n",
      " 'ai1s' 'ai2' 'ai2s' 'aible' 'aid' 'aidan' 'aide' 'aided' 'aiden' 'aides'\n",
      " 'aiding' 'aidleyco' 'aids' 'ail' 'ailes' 'ailing' 'ailment' 'ailments'\n",
      " 'ails' 'aim' 'aimed' 'aiming' 'aimlessly' 'aims' 'ain' 'ainda' 'aint'\n",
      " 'aio' 'aios' 'aiothirsties' 'aiportextreme' 'air' 'airbag' 'airbags'\n",
      " 'airbed' 'airbeds' 'airboat' 'airborn' 'airborne' 'airbrush' 'airbrushed'\n",
      " 'airconditioning' 'aircraft' 'aire' 'aired' 'airedale' 'aires' 'airflow'\n",
      " 'airfresher' 'airfreshner' 'airhole' 'airier' 'airing' 'airline'\n",
      " 'airlines' 'airlocked' 'airmen' 'airobot' 'airout' 'airplane' 'airplanes'\n",
      " 'airplenaes' 'airpockets' 'airport' 'airportexpress' 'airportextreme'\n",
      " 'airporti' 'airports' 'airportscons' 'airpot' 'airprotect' 'airs'\n",
      " 'airspace' 'airstraps' 'airstream' 'airtight' 'airvent' 'airvents'\n",
      " 'airwave' 'airway' 'airways' 'airy' 'airyness' 'aisi' 'aislamiento'\n",
      " 'aisle' 'aisled' 'aisles' 'aislescould' 'ait' 'aj' 'ajar' 'ajoining'\n",
      " 'ajust' 'ajusta' 'ajustable' 'ajuste' 'ajusters' 'ajustment' 'ajustments'\n",
      " 'ak' 'aka' 'akacabinet' 'ake' 'akiddopotamus' 'akin' 'akita' 'aknow'\n",
      " 'akward' 'akwardly' 'akwardness' 'al' 'al2' 'ala' 'alaa' 'alabama'\n",
      " 'alaina' 'alalska' 'alam' 'alan' 'alana' 'alano' 'alapaha' 'alarm'\n",
      " 'alarmed' 'alarming' 'alarmingly' 'alarmist' 'alarms' 'alas' 'alaska'\n",
      " 'alaskan' 'alaskian' 'alaundry' 'alaways' 'alawys' 'alba' 'albee'\n",
      " 'albeebaby' 'albeit' 'albert' 'alberta' 'albertson' 'albertsons' 'albino'\n",
      " 'album' 'albums' 'albun' 'albuquerque' 'alcanza' 'alchohal' 'alchol'\n",
      " 'alcohol' 'alcoholed' 'alcoholic' 'alcove' 'alder' 'aldi' 'aldis' 'aldo'\n",
      " 'aldow' 'ale' 'aleady' 'aleays' 'aleep' 'alera' 'alergic' 'alergico'\n",
      " 'alero' 'alert' 'alerted' 'alerting' 'alertness' 'alerts' 'ales'\n",
      " 'aleviate' 'aleviated' 'alex' 'alexander' 'alexia' 'alfhabet' 'algae'\n",
      " 'algo' 'algorithm' 'alhtough' 'ali' 'alice' 'alien' 'aliens' 'aligator'\n",
      " 'alighting' 'align' 'aligned' 'alignement' 'aligning' 'alignment'\n",
      " 'aligns' 'alike' 'alil' 'alimentacion' 'alimentar' 'alimentary'\n",
      " 'alimentos' 'alimentum' 'alined' 'aliphatic' 'alison' 'alissa' 'alittel'\n",
      " 'alittle' 'alive' 'alk' 'alka' 'alkaline' 'alkalines' 'alked' 'alking'\n",
      " 'all' 'allaboutclothdiapers' 'allan' 'allarming' 'allday' 'alldislikes'\n",
      " 'alldoes' 'alleaviate' 'alledged' 'alleged' 'allegedly' 'allegiance'\n",
      " 'allegiant' 'allegric' 'alleluia' 'allemande' 'allen' 'allens' 'allergan'\n",
      " 'allergen' 'allergenic' 'allergens' 'allergic' 'allergies' 'allergist'\n",
      " 'allergy' 'alleviate' 'alleviated' 'alleviates' 'alleviating' 'alley'\n",
      " 'alli' 'alliance' 'alligator' 'alligators' 'allison' 'alliteration'\n",
      " 'allived' 'alll' 'alllightweight' 'allll' 'alllll' 'alllllllways'\n",
      " 'allmodern' 'allmomsareperfect' 'allmost' 'allmy' 'allnight' 'allno'\n",
      " 'allo' 'alloew' 'allof' 'allot' 'alloted' 'allotted' 'allouette'\n",
      " 'allover' 'alloverall' 'allow' 'allowable' 'allowance' 'allowances'\n",
      " 'allowed' 'allowin' 'allowing' 'allowopen' 'allows' 'alloy' 'allplaytex'\n",
      " 'allready' 'allright' 'allroad' 'alls' 'allshould' 'allthat' 'allthe'\n",
      " 'alltogether' 'alluded' 'alludes' 'allure' 'alluring' 'allways'\n",
      " 'allwoing' 'allworks' 'allyn' 'alma' 'almay' 'almighty' 'almkst'\n",
      " 'almohada' 'almohaditas' 'almond' 'almonds' 'almos' 'almose' 'almost'\n",
      " 'almost11' 'almosy' 'almosyw' 'alocohol' 'aloe' 'aloha' 'aloi' 'alomost'\n",
      " 'alomst' 'alone' 'alone3' 'alone7' 'alone8' 'alonecons' 'alones'\n",
      " 'alonewhat' 'along' 'alongs' 'alongside' 'alongsidesidekick' 'aloows'\n",
      " 'alopecia' 'alos' 'alot' 'alota' 'alots' 'alotted' 'alotuseful' 'aloud'\n",
      " 'alovingmom' 'alowe' 'alowing' 'alows' 'alpaca' 'alpha' 'alphabet'\n",
      " 'alphabeth' 'alphabetical' 'alphabetically']\n",
      "(57381,)\n",
      "(133402, 57381)\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nsize is too big, so we cant do .todense\\nInstead I print small portion of dictionary\\n'"
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b)Transform reviews into vectors using CountVectorizer. \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_example = vectorizer.fit_transform(train_df.review)\n",
    "# only transform on test\n",
    "\n",
    "print(vectorizer.get_feature_names_out()[3000:4000])\n",
    "# print(X_train_example.todense())\n",
    "print(vectorizer.get_feature_names_out().shape)\n",
    "print(X_train_example.shape) #too big to do todense()\n",
    "\"\"\"\n",
    "size is too big, so we cant do .todense\n",
    "Instead I print small portion of dictionary\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 \n",
    "a) Train LogisticRegression model on training data (reviews processed with CountVectorizer, ratings as they were).   \n",
    "b) Print 10 most positive and 10 most negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:50:54.378143Z",
     "start_time": "2023-12-18T21:50:25.700206Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#a) Train LogisticRegression model on training data (reviews processed with CountVectorizer, ratings as they were)\n",
    "\"\"\"\n",
    "I had to change max_iter because model was failing do converge\n",
    "\"\"\"\n",
    "no_dict_train_0 = time.time()\n",
    "model = LogisticRegression(max_iter=1200)\n",
    "model.fit(X_train_example, train_df[\"rating\"])\n",
    "no_dict_train_1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:50:54.611993Z",
     "start_time": "2023-12-18T21:50:54.440170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excellent', 'skeptical', 'pleased', 'saves', 'outstanding', 'highly', 'pleasantly', 'worry', 'amazed', 'rich']\n",
      "['dissapointed', 'worst', 'worthless', 'poorly', 'intelligent', 'unusable', 'disappointing', 'concept', 'falsely', 'poor']\n"
     ]
    }
   ],
   "source": [
    "#b) Print 10 most positive and 10 most negative words\n",
    "sorted_coef, sorted_features = (list(x) for x in zip(*sorted(zip(model.coef_.tolist()[0], vectorizer.get_feature_names_out()))))\n",
    "\n",
    "top_positive = sorted_features[-10:]\n",
    "top_negative = sorted_features[:10]\n",
    "\n",
    "print(top_positive)\n",
    "print(top_negative)\n",
    "#hint: model.coef_, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 \n",
    "a) Predict the sentiment of test data reviews.   \n",
    "b) Predict the sentiment of test data reviews in terms of probability.   \n",
    "c) Find five most positive and most negative reviews.   \n",
    "d) Calculate the accuracy of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:50:55.957453Z",
     "start_time": "2023-12-18T21:50:54.618336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1 -1  1  1 -1  1  1  1  1  1  1  1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "#a)\n",
    "# Here I predict sentiment od test data reviews.\n",
    "# As we can observe on first 20 reviews, they can be either positive or negative.\n",
    "X_test_example = vectorizer.transform(test_df.review) # create test data vectorizer\n",
    "no_dict_predict_0 = time.time()\n",
    "sentiments = model.predict(X_test_example)\n",
    "no_dict_predict_1 = time.time()\n",
    "print(sentiments[:20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:50:55.966801Z",
     "start_time": "2023-12-18T21:50:55.958708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: They like bananas\n",
      "Probability for negative sentence: 0.00797114819307787\n",
      "Probability for positive sentence: 0.9920288518069221\n",
      "\n",
      "Sentence: We love bananas\n",
      "Probability for negative sentence: 0.05755111956822989\n",
      "Probability for positive sentence: 0.9424488804317701\n"
     ]
    }
   ],
   "source": [
    "#b)\n",
    "# here I have written with what probability a sentiment is given.\n",
    "# it prints the probability for positive or negative result\n",
    "probability_reviews_sentiment = model.predict_proba(X_test_example)\n",
    "\n",
    "print(f\"Sentence: {reviews_test_example[0]}\\n\"\n",
    "      f\"Probability for negative sentence: \"\n",
    "      f\"{probability_reviews_sentiment[0][0]}\\n\"\n",
    "      f\"Probability for positive sentence: \"\n",
    "      f\"{probability_reviews_sentiment[0][1]}\\n\")\n",
    "\n",
    "# As we can see, prediction is very strong, that te given sentence has a positive meaning\n",
    "\n",
    "print(f\"Sentence: {reviews_test_example[2]}\\n\"\n",
    "      f\"Probability for negative sentence: \"\n",
    "      f\"{probability_reviews_sentiment[2][0]}\\n\"\n",
    "      f\"Probability for positive sentence: \"\n",
    "      f\"{probability_reviews_sentiment[2][1]}\")\n",
    "\n",
    "# In second example, with different sentence we can also observe high probability for positive meaning and in fact, this sentence has positive meaning\n",
    "# Predictions are working!\n",
    "\n",
    "#hint: model.predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:50:56.010326Z",
     "start_time": "2023-12-18T21:50:56.006522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"These bowls are nice and sturdy, just like I expected them to be.  They are just the right size for my daughter's lunch.  The only issue is that the lids are somewhat hard to get on.  Just takes a little practice, though.  I definitely think we'll get more than our money's worth out of these.\", 'These bottles are excellent bottles for to prevent colic in babies.  The price is to expensive.', 'Its cool, refuses to stay on my faucet though as my facuet starts small and gets bigger... Still Trying to find a way to rig it up to stay on', 'Of course, no accidents since we purchased this.  But the sale was easy, the pad was easy to put on.', 'I think by far this is the best rattle we own. The handle is small enough that a 2 month old starting to learn to grasp can practice on, they can chew on it ( which they will all eventually do), the rattle sound is loud enough to get their attention without annoying the grown ups in the room. It is colorful enough that they can learn to follow it when you dangle it. Over-all just a great product']\n",
      "\n",
      "\n",
      "\n",
      "[\"This blanket is almost more like a playmat, in that it's so fluffy thick that it seems more suited to laying on rather than under---it's beautiful and soft and nice quality.  I like it!\", \"I purchased this to protect my daughter's neck from her car seat straps in the summer when she's wearing sleeveless shirts or dresses and it's perfect!  She's much happier without the straps digging into her skin.  They're super easy to put on and off and cute as can be! I highly recommend this product!\", 'I bought one of these for a \"well-endowed\" friend who was expecting. It was the hit of the baby shower, and now that her daughter is here, my friend reports that both she and the baby are loving the nursing system!', 'Failed to post this review when purchased a couple years ago.  My son is now 4 & still fits in this seat.  He has nearly outgrown it but it still fits him (for now).  I feel that this seat is extremely safe & worth the extra $.  I have recommended this seat to everyone that I know who is either expecting a child or moving their child out of the infant seat.  Although this seat can be used for a newborn, I would recommend the carseat/carrier style for easy baby transport to & from the car.  I put my son in this seat once he outgrew his infant carrier seat.', \"Did I say great?  This was our second monitor.  Our first would make sounds from the unit in the baby's room and on our unit.  I was very annoyed.  This unit is silent in the baby's room and had Zero feedback from our cell phone, cordless phone, and other electrical equipment.  It also amplifies the sound and is very clear.  Our baby has a very soft cry and the volume on this unit could be adjusted enough to wake us up out of the soundest of sleeps.\"]\n"
     ]
    }
   ],
   "source": [
    "#c) Find five most positive and most negative reviews.\n",
    "sorted_sentiment, sorted_reviews = (list(x) for x in zip(*sorted(zip(probability_reviews_sentiment[:,0], baby_df[\"review\"]), reverse=True)))\n",
    "# sorting in descending order - best review is first\n",
    "\n",
    "top_positive_review = sorted_reviews[0:5]\n",
    "top_negative_review = sorted_reviews[-6:-1]\n",
    "\n",
    "print(top_positive_review, end=\"\\n\\n\\n\\n\")\n",
    "print(top_negative_review)\n",
    "\n",
    "# I am having problems with extracting real positive and negative reviews.\n",
    "# The problem could be that people who write negative reviews tend to write them short,\n",
    "# and the model definitely favours long reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:50:56.029197Z",
     "start_time": "2023-12-18T21:50:56.011212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9491554997208264\n"
     ]
    }
   ],
   "source": [
    "#d) Calculate the accuracy of predictions.\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "y_pred = model.predict(X_test_example)\n",
    "y_true = test_df[\"rating\"]\n",
    "\n",
    "first_score = precision_score(y_true,  y_pred)\n",
    "print(first_score)\n",
    "\n",
    "# accuracy of predictions is almost 95 %, it is very good result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "In this exercise we will limit the dictionary of CountVectorizer to the set of significant words, defined below.\n",
    "\n",
    "\n",
    "a) Redo exercises 2-5 using limited dictionary.   \n",
    "b) Check the impact of all the words from the dictionary.   \n",
    "c) Compare accuracy of predictions and the time of evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:50:56.058441Z",
     "start_time": "2023-12-18T21:50:56.029595Z"
    }
   },
   "outputs": [],
   "source": [
    "significant_words = ['love','great','easy','old','little','perfect','loves','well','able','car','broke','less','even','waste','disappointed','work','product','money','would','return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:51:02.147620Z",
     "start_time": "2023-12-18T21:50:56.038025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:  ['car', 'old', 'able', 'little', 'well', 'great', 'easy', 'love', 'perfect', 'loves']\n",
      "Negative:  ['disappointed', 'return', 'waste', 'broke', 'money', 'work', 'even', 'would', 'product', 'less']\n"
     ]
    }
   ],
   "source": [
    "#a) I don't split my data since I already did that.\n",
    "\n",
    "train_df = baby_df.sample(frac=.8, random_state=42)\n",
    "test_df = baby_df.drop(train_df.index)\n",
    "\n",
    "vectorizer_subset = CountVectorizer()\n",
    "\n",
    "vectorizer_subset.fit_transform(significant_words)\n",
    "\n",
    "X_train_subset = vectorizer_subset.transform(train_df.review)\n",
    "X_test_subset = vectorizer_subset.transform(test_df.review)\n",
    "\n",
    "with_dict_train_0 = time.time()\n",
    "second_model = LogisticRegression(max_iter=1200)\n",
    "second_model.fit(X_train_subset,  train_df[\"rating\"])\n",
    "with_dict_train_1 = time.time()\n",
    "\n",
    "sorted_coef_subset, sorted_features_subset = (list(t) for t in zip(*sorted(zip(second_model.coef_.tolist()[0], vectorizer_subset.get_feature_names_out()))))\n",
    "\n",
    "top_positive = sorted_features_subset[-10:]\n",
    "top_negative = sorted_features_subset[:10]\n",
    "\n",
    "print('Positive: ', top_positive)\n",
    "print('Negative: ', top_negative)\n",
    "\n",
    "# find X_train_subset; X_test_subset, using vectorizer_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "with_dict_predict_0 = time.time()\n",
    "sentiments = second_model.predict(X_test_subset)\n",
    "with_dict_predict_1 = time.time()\n",
    "print(sentiments[:20])\n",
    "\n",
    "# Here we receive the same result as before\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:51:02.152111Z",
     "start_time": "2023-12-18T21:51:02.148157Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: They like bananas\n",
      "Probability for negative sentence: 0.18146233040711224\n",
      "Probability for positive sentence: 0.8185376695928878\n",
      "\n",
      "Sentence: We love bananas\n",
      "Probability for negative sentence: 0.21641058825873238\n",
      "Probability for positive sentence: 0.7835894117412676\n"
     ]
    }
   ],
   "source": [
    "probability_reviews_sentiment = second_model.predict_proba(X_test_subset)\n",
    "\n",
    "print(f\"Sentence: {reviews_test_example[0]}\\n\"\n",
    "      f\"Probability for negative sentence: \"\n",
    "      f\"{probability_reviews_sentiment[0][0]}\\n\"\n",
    "      f\"Probability for positive sentence: \"\n",
    "      f\"{probability_reviews_sentiment[0][1]}\\n\")\n",
    "\n",
    "# As we can see, prediction is very strong, that te given sentence has a positive meaning\n",
    "\n",
    "print(f\"Sentence: {reviews_test_example[2]}\\n\"\n",
    "      f\"Probability for negative sentence: \"\n",
    "      f\"{probability_reviews_sentiment[2][0]}\\n\"\n",
    "      f\"Probability for positive sentence: \"\n",
    "      f\"{probability_reviews_sentiment[2][1]}\")\n",
    "\n",
    "# In this case, model is not as sure as in the previous example\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:51:02.163590Z",
     "start_time": "2023-12-18T21:51:02.153155Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is great for calming the fears of a little one when bedtime is near.  Lovely display covers the ceiling.', \"It's great! I love it! My niece loves it! Her parents love it! All my family love it! It's great for kids!\", 'I love this product. When I prepare food for my baby, I feel so happy.Yes. It is right. You have to add some water in there to get better results. However, I just broke the gasket. It is still working but it is leaking a little bit:( I do not know whether I can find the gasket!I guess it was my fault so I still recommend this product.', \"We love this bath tub!  We had the First Year convertible bath tub first...horrible.  Our son slipped all over the place and you could not get all areas cleaned because of the set up of the tub.  We love the Euro Bath!  It is large enough for baby to have splash and play time without worrying about them slipping.  I love the fact that our son can continue to grow with this tub!  Don't worry about buying anything else!!!!\", 'This hamper matches my daughter\\'s room perfectly and she loves \"helping\" me do her laundry.  It\\'s short enough she can put the clothes in it herself.  I don\\'t let her play with it though. Not quite sturdy enough when she trys to lay on it.  It serves it\\'s purpose of looking good in the room, while being a functional.']\n",
      "\n",
      "\n",
      "\n",
      "['My daughter LOVES this mobile!  When she was a newborn, I felt like I had to always be holding her.  But this mobile gave me a much needed break to get other things done, even if just for a few minutes.  She loves the different shapes and colors and will study at it and smile and flap her arms enthusiastically.  It definitely holds her interest and is visually stimulating.', \"My husband installed these and he was irritated by how long it took.  Other than that, though, I'm happy with the way they work.  My ten-month-old can pull the drawers out, and then yank and push on them without getting her fingers pinched.  As far as this type of lock goes, I'm pretty happy with them!\", 'I bought one of these for a \"well-endowed\" friend who was expecting. It was the hit of the baby shower, and now that her daughter is here, my friend reports that both she and the baby are loving the nursing system!', 'This toy has a lot of little features on it. It is perfect for little hands and to be able to hang on the carseat.', 'This car seat has been great, my daughter is very comfortable in it.  We bought the boster with a high back because it has a head rest for long trips.  She sleeps just fine in this one because the head sides turn in more than other seats. It folds for storage and is easy to travel with, I can fold it and carry it through the airport easily because it is so light.  I was concerned about the light color getting dirty quickly but it is washable.']\n"
     ]
    }
   ],
   "source": [
    "proba_sentiments = second_model.predict_proba(X_test_subset)\n",
    "\n",
    "sorted_sentiment, sorted_reviews = (list(x) for x in zip(*sorted(zip(proba_sentiments[:,0], baby_df[\"review\"]), reverse=True)))\n",
    "# sorting in descending order - best review is first\n",
    "\n",
    "top_positive_review = sorted_reviews[0:5]\n",
    "top_negative_review = sorted_reviews[-6:-1]\n",
    "\n",
    "print(top_positive_review, end=\"\\n\\n\\n\\n\")\n",
    "print(top_negative_review)\n",
    "\n",
    "#same as in exercise 4, I have no idea why I receive wrong output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:51:02.226345Z",
     "start_time": "2023-12-18T21:51:02.158759Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:51:02.226586Z",
     "start_time": "2023-12-18T21:51:02.207380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "able 0.21976363349928654\n",
      "broke -1.6703338320502181\n",
      "car 0.05753770019024416\n",
      "disappointed -2.3983054566077855\n",
      "easy 1.1451412454555527\n",
      "even -0.5168710532218316\n",
      "great 0.9483502452216717\n",
      "less -0.15665853311237765\n",
      "little 0.49983953018095795\n",
      "love 1.3566339961091352\n",
      "loves 1.714153155127098\n",
      "money -0.878863514014164\n",
      "old 0.09387289540906824\n",
      "perfect 1.5331518579535348\n",
      "product -0.3212417688019006\n",
      "return -2.0672539101190512\n",
      "waste -2.0128900120635658\n",
      "well 0.5282632108466655\n",
      "work -0.6163190781420029\n",
      "would -0.33534385634315766\n"
     ]
    }
   ],
   "source": [
    "#b)\n",
    "\n",
    "for coef, feature in zip(second_model.coef_[0], vectorizer_subset.get_feature_names_out()):\n",
    "    print(feature, coef)\n",
    "    \n",
    "    \n",
    "# I have no idea why, but I receive wrong results in some cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:51:02.226673Z",
     "start_time": "2023-12-18T21:51:02.210451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, no dict: 28.676074981689453\n",
      "Predict, no dict: 0.005292177200317383\n",
      "Train, dict: 0.19220709800720215\n",
      "Predict, dict: 0.0012595653533935547\n"
     ]
    }
   ],
   "source": [
    "#c)\n",
    "\n",
    "print(f'Train, no dict: {no_dict_train_1 - no_dict_train_0}\\n'\n",
    "      f'Predict, no dict: {no_dict_predict_1 - no_dict_predict_0}\\n'\n",
    "      f'Train, dict: {with_dict_train_1 - with_dict_train_0}\\n'\n",
    "      f'Predict, dict: {with_dict_predict_1 - with_dict_predict_0}')\n",
    "\n",
    "# as we can bserve in output, training with dictionary is way quicker than without\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8736418638537806\n"
     ]
    }
   ],
   "source": [
    "# comparing checking accuracy of prediction\n",
    "y_pred = second_model.predict(X_test_subset)\n",
    "y_true = test_df[\"rating\"]\n",
    "\n",
    "second_score = precision_score(y_true,  y_pred)\n",
    "\n",
    "print(second_score)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:51:02.243116Z",
     "start_time": "2023-12-18T21:51:02.214301Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First model was more accurate\n"
     ]
    }
   ],
   "source": [
    "if first_score > second_score:\n",
    "    print(\"First model was more accurate\")\n",
    "else:\n",
    "    print(\"Second model was more accurate\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:51:02.244968Z",
     "start_time": "2023-12-18T21:51:02.228470Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:51:02.245158Z",
     "start_time": "2023-12-18T21:51:02.231387Z"
    }
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
