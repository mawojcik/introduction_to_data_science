{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When exploring a large set of documents -- such as Wikipedia, news articles, StackOverflow, etc. -- it can be useful to get a list of related material. To find relevant documents you typically\n",
    "* Decide on a notion of similarity\n",
    "* Find the documents that are most similar \n",
    "\n",
    "In the assignment you will\n",
    "* Gain intuition for different notions of similarity and practice finding similar documents. \n",
    "* Explore the tradeoffs with representing documents using raw word counts and TF-IDF\n",
    "* Explore the behavior of different distance metrics by looking at the Wikipedia pages most similar to President Obama’s page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:07:10.295883Z",
     "start_time": "2023-12-31T11:07:09.912395Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Wikipedia dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the dataset of abridged Wikipedia pages. Each element of the dataset consists of a link to the wikipedia article, the name of the person, and the text of the article (in lowercase).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:07:11.040775Z",
     "start_time": "2023-12-31T11:07:09.914940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 URI                 name  \\\n0        <http://dbpedia.org/resource/Digby_Morrell>        Digby Morrell   \n1       <http://dbpedia.org/resource/Alfred_J._Lewy>       Alfred J. Lewy   \n2        <http://dbpedia.org/resource/Harpdog_Brown>        Harpdog Brown   \n3  <http://dbpedia.org/resource/Franz_Rottensteiner>  Franz Rottensteiner   \n4               <http://dbpedia.org/resource/G-Enka>               G-Enka   \n\n                                                text  \n0  digby morrell born 10 october 1979 is a former...  \n1  alfred j lewy aka sandy lewy graduated from un...  \n2  harpdog brown is a singer and harmonica player...  \n3  franz rottensteiner born in waidmannsfeld lowe...  \n4  henry krvits born 30 december 1974 in tallinn ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>URI</th>\n      <th>name</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;http://dbpedia.org/resource/Digby_Morrell&gt;</td>\n      <td>Digby Morrell</td>\n      <td>digby morrell born 10 october 1979 is a former...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;http://dbpedia.org/resource/Alfred_J._Lewy&gt;</td>\n      <td>Alfred J. Lewy</td>\n      <td>alfred j lewy aka sandy lewy graduated from un...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;http://dbpedia.org/resource/Harpdog_Brown&gt;</td>\n      <td>Harpdog Brown</td>\n      <td>harpdog brown is a singer and harmonica player...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;http://dbpedia.org/resource/Franz_Rottensteiner&gt;</td>\n      <td>Franz Rottensteiner</td>\n      <td>franz rottensteiner born in waidmannsfeld lowe...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;http://dbpedia.org/resource/G-Enka&gt;</td>\n      <td>G-Enka</td>\n      <td>henry krvits born 30 december 1974 in tallinn ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = pd.read_csv('people_wiki.csv')\n",
    "wiki.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to check whether the text on the webpage agrees with the one here, you can display it with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:07:11.040944Z",
     "start_time": "2023-12-31T11:07:10.942Z"
    }
   },
   "outputs": [],
   "source": [
    "# from IPython.display import HTML\n",
    "# print(wiki['text'][0])\n",
    "# HTML(url=wiki['URI'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex. 1: Extract word count vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in Assignment 4, we can extract word count vectors using `CountVectorizer` function.\n",
    "- make sure you include words of unit length by using the parameter: `token_pattern=r\"(?u)\\b\\w+\\b\"`\n",
    "- do not use any stopwords\n",
    "- take 10000 most frequent words in the corpus\n",
    "- explicitly take all the words independent of in how many documents they occur\n",
    "- obtain the matrix of word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:07:27.272439Z",
     "start_time": "2023-12-31T11:07:11.075071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 117)\t1\n",
      "  (0, 161)\t1\n",
      "  (0, 168)\t1\n",
      "  (0, 172)\t1\n",
      "  (0, 176)\t1\n",
      "  (0, 185)\t1\n",
      "  (0, 190)\t1\n",
      "  (0, 195)\t1\n",
      "  (0, 200)\t1\n",
      "  (0, 205)\t1\n",
      "  (0, 216)\t1\n",
      "  (0, 226)\t1\n",
      "  (0, 229)\t1\n",
      "  (0, 234)\t1\n",
      "  (0, 265)\t1\n",
      "  (0, 372)\t4\n",
      "  (0, 454)\t1\n",
      "  (0, 544)\t1\n",
      "  (0, 556)\t2\n",
      "  (0, 557)\t1\n",
      "  (0, 664)\t1\n",
      "  (0, 670)\t1\n",
      "  (0, 722)\t4\n",
      "  (0, 903)\t2\n",
      "  :\t:\n",
      "  (59070, 8555)\t1\n",
      "  (59070, 8625)\t1\n",
      "  (59070, 8628)\t1\n",
      "  (59070, 8629)\t2\n",
      "  (59070, 8642)\t1\n",
      "  (59070, 8656)\t1\n",
      "  (59070, 8666)\t1\n",
      "  (59070, 8823)\t1\n",
      "  (59070, 8963)\t1\n",
      "  (59070, 8978)\t1\n",
      "  (59070, 9038)\t1\n",
      "  (59070, 9060)\t2\n",
      "  (59070, 9063)\t14\n",
      "  (59070, 9111)\t1\n",
      "  (59070, 9120)\t2\n",
      "  (59070, 9142)\t1\n",
      "  (59070, 9162)\t6\n",
      "  (59070, 9441)\t2\n",
      "  (59070, 9449)\t1\n",
      "  (59070, 9579)\t2\n",
      "  (59070, 9704)\t8\n",
      "  (59070, 9751)\t1\n",
      "  (59070, 9774)\t3\n",
      "  (59070, 9788)\t1\n",
      "  (59070, 9860)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "    max_features=10000,\n",
    ")\n",
    "vectorizer.fit_transform(wiki[\"text\"])\n",
    "WCmatrix = vectorizer.transform(wiki[\"text\"])\n",
    "print(WCmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ex. 2: Find nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Start by finding the nearest neighbors of the Barack Obama page using the above word count matrix to represent the articles and **Euclidean** distance to measure distance.\n",
    "Save the distances in `wiki['BO-eucl']` and look at the top 10 nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:07:27.376338Z",
     "start_time": "2023-12-31T11:07:27.279199Z"
    }
   },
   "outputs": [],
   "source": [
    "# One can use the following:\n",
    "    # from sklearn.neighbors import NearestNeighbors\n",
    "    # nbrs = NearestNeighbors(n_neighbors=3, algorithm='brute',metric='euclidean').fit(X.toarray())\n",
    "    # distances, indices = nbrs.kneighbors(X.toarray())\n",
    "# but here let's use:\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "dist = pairwise_distances(WCmatrix, WCmatrix[wiki[\"name\"] == 'Barack Obama'], metric='euclidean', n_jobs=-1)\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:07:27.395953Z",
     "start_time": "2023-12-31T11:07:27.362163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   URI               name  \\\n35817       <http://dbpedia.org/resource/Barack_Obama>       Barack Obama   \n24478          <http://dbpedia.org/resource/Joe_Biden>          Joe Biden   \n28447     <http://dbpedia.org/resource/George_W._Bush>     George W. Bush   \n48202       <http://dbpedia.org/resource/Tony_Vaccaro>       Tony Vaccaro   \n14754        <http://dbpedia.org/resource/Mitt_Romney>        Mitt Romney   \n31423     <http://dbpedia.org/resource/Walter_Mondale>     Walter Mondale   \n36364         <http://dbpedia.org/resource/Don_Bonker>         Don Bonker   \n13229   <http://dbpedia.org/resource/Francisco_Barrio>   Francisco Barrio   \n35357   <http://dbpedia.org/resource/Lawrence_Summers>   Lawrence Summers   \n25258  <http://dbpedia.org/resource/Marc_Ravalomanana>  Marc Ravalomanana   \n\n                                                    text    BO-eucl  \n35817  barack hussein obama ii brk husen bm born augu...   0.000000  \n24478  joseph robinette joe biden jr dosf rbnt badn b...  31.336879  \n28447  george walker bush born july 6 1946 is an amer...  33.645208  \n48202  michelantonio celestino onofrio vaccaro born d...  33.734256  \n14754  willard mitt romney born march 12 1947 is an a...  34.351128  \n31423  walter frederick fritz mondale born january 5 ...  34.423829  \n36364  don leroy bonker born march 7 1937 in denver c...  34.597688  \n13229  francisco javier barrio terrazas born november...  34.669872  \n35357  lawrence henry larry summers born november 30 ...  35.383612  \n25258  marc ravalomanana malagasy ravalumanan born 12...  35.440090  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>URI</th>\n      <th>name</th>\n      <th>text</th>\n      <th>BO-eucl</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>35817</th>\n      <td>&lt;http://dbpedia.org/resource/Barack_Obama&gt;</td>\n      <td>Barack Obama</td>\n      <td>barack hussein obama ii brk husen bm born augu...</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>24478</th>\n      <td>&lt;http://dbpedia.org/resource/Joe_Biden&gt;</td>\n      <td>Joe Biden</td>\n      <td>joseph robinette joe biden jr dosf rbnt badn b...</td>\n      <td>31.336879</td>\n    </tr>\n    <tr>\n      <th>28447</th>\n      <td>&lt;http://dbpedia.org/resource/George_W._Bush&gt;</td>\n      <td>George W. Bush</td>\n      <td>george walker bush born july 6 1946 is an amer...</td>\n      <td>33.645208</td>\n    </tr>\n    <tr>\n      <th>48202</th>\n      <td>&lt;http://dbpedia.org/resource/Tony_Vaccaro&gt;</td>\n      <td>Tony Vaccaro</td>\n      <td>michelantonio celestino onofrio vaccaro born d...</td>\n      <td>33.734256</td>\n    </tr>\n    <tr>\n      <th>14754</th>\n      <td>&lt;http://dbpedia.org/resource/Mitt_Romney&gt;</td>\n      <td>Mitt Romney</td>\n      <td>willard mitt romney born march 12 1947 is an a...</td>\n      <td>34.351128</td>\n    </tr>\n    <tr>\n      <th>31423</th>\n      <td>&lt;http://dbpedia.org/resource/Walter_Mondale&gt;</td>\n      <td>Walter Mondale</td>\n      <td>walter frederick fritz mondale born january 5 ...</td>\n      <td>34.423829</td>\n    </tr>\n    <tr>\n      <th>36364</th>\n      <td>&lt;http://dbpedia.org/resource/Don_Bonker&gt;</td>\n      <td>Don Bonker</td>\n      <td>don leroy bonker born march 7 1937 in denver c...</td>\n      <td>34.597688</td>\n    </tr>\n    <tr>\n      <th>13229</th>\n      <td>&lt;http://dbpedia.org/resource/Francisco_Barrio&gt;</td>\n      <td>Francisco Barrio</td>\n      <td>francisco javier barrio terrazas born november...</td>\n      <td>34.669872</td>\n    </tr>\n    <tr>\n      <th>35357</th>\n      <td>&lt;http://dbpedia.org/resource/Lawrence_Summers&gt;</td>\n      <td>Lawrence Summers</td>\n      <td>lawrence henry larry summers born november 30 ...</td>\n      <td>35.383612</td>\n    </tr>\n    <tr>\n      <th>25258</th>\n      <td>&lt;http://dbpedia.org/resource/Marc_Ravalomanana&gt;</td>\n      <td>Marc Ravalomanana</td>\n      <td>marc ravalomanana malagasy ravalumanan born 12...</td>\n      <td>35.440090</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving the distances in wiki[\"BO-eucl\"] and printing top 10 nearest neighbours\n",
    "\n",
    "wiki[\"BO-eucl\"] = dist\n",
    "wiki = wiki.sort_values(by='BO-eucl')\n",
    "wiki.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Measure the pairwise distance between the Wikipedia pages of Barack Obama, George W. Bush, and Joe Biden. Which of the three pairs has the smallest distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:07:27.439670Z",
     "start_time": "2023-12-31T11:07:27.399011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bush-Biden: 30.919249667480614\n",
      "Bush-Obama: 33.645207682521445\n",
      "Obama-Biden: 31.336879231984796\n"
     ]
    }
   ],
   "source": [
    "george_bush_row = wiki[wiki[\"name\"] == \"George W. Bush\"].index[0]\n",
    "joe_biden_row = wiki[wiki[\"name\"] == \"Joe Biden\"].index[0]\n",
    "barack_obama_row = wiki[wiki[\"name\"] == \"Barack Obama\"].index[0]\n",
    "\n",
    "def find_distance(index1: int, index2: int) -> int:\n",
    "    return pairwise_distances(WCmatrix[index1].toarray(), WCmatrix[index2].toarray())[0, 0]\n",
    "\n",
    "print(f'Bush-Biden: {find_distance(george_bush_row, joe_biden_row)}')\n",
    "print(f'Bush-Obama: {find_distance(george_bush_row, barack_obama_row)}')\n",
    "print(f'Obama-Biden: {find_distance(barack_obama_row, joe_biden_row)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the 10 people from **a)** are politicians, but about half of them have rather tenuous connections with Obama, other than the fact that they are politicians, e.g.,\n",
    "\n",
    "* Francisco Barrio is a Mexican politician, and a former governor of Chihuahua.\n",
    "* Walter Mondale and Don Bonker are Democrats who made their career in late 1970s.\n",
    "\n",
    "Nearest neighbors with raw word counts got some things right, showing all politicians in the query result, but missed finer and important details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Let's find out why Francisco Barrio was considered a close neighbor of Obama.\n",
    "To do this, look at the most frequently used words in each of Barack Obama and Francisco Barrio's pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:07:27.439806Z",
     "start_time": "2023-12-31T11:07:27.402724Z"
    }
   },
   "outputs": [],
   "source": [
    "def top_words(name):\n",
    "    row = wiki[wiki[\"name\"] == name].index[0]\n",
    "    word_count = WCmatrix[row].toarray()[0]\n",
    "    df = pd.DataFrame(data={'count': word_count}, index=vectorizer.get_feature_names_out())\n",
    "    df = df[df['count'] != 0]\n",
    "    return df.sort_values(by='count',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:07:27.440146Z",
     "start_time": "2023-12-31T11:07:27.414551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          count\nthe          40\nin           30\nand          21\nof           18\nto           14\n...         ...\nhawaii        1\nhillary       1\nhold          1\nhonolulu      1\nyears         1\n\n[245 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>the</th>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>in</th>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>of</th>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>to</th>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>hawaii</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>hillary</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>hold</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>honolulu</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>years</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>245 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_words = top_words('Barack Obama')\n",
    "obama_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:07:27.511175Z",
     "start_time": "2023-12-31T11:07:27.418024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            count\nthe            36\nof             24\nand            18\nin             17\nhe             10\n...           ...\ngovernance      1\ngovernors       1\nhas             1\nheaded          1\njoining         1\n\n[195 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>the</th>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>of</th>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>in</th>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>he</th>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>governance</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>governors</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>has</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>headed</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>joining</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>195 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barrio_words = top_words('Francisco Barrio')\n",
    "barrio_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Extract the list of most frequent **common** words that appear in both Obama's and Barrio's documents and display the five words that appear most often in Barrio's article.\n",
    "\n",
    "Use a dataframe operation known as **join**. The **join** operation is very useful when it comes to playing around with data: it lets you combine the content of two tables using a shared column (in this case, the index column of words). See [the documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:07:27.511383Z",
     "start_time": "2023-12-31T11:07:27.434404Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modify the code to avoid error.\n",
    "\n",
    "common_words = obama_words.join(barrio_words, how='inner', lsuffix='_Obama', rsuffix='_Barrio').dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all words that appear both in Barack Obama and George W. Bush pages.  Out of those words, find the 10 words that show up most often in Obama's page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:07:27.571087Z",
     "start_time": "2023-12-31T11:07:27.447031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     count_Obama  count_Bush\nthe           40          39\nin            30          22\nand           21          14\nof            18          14\nto            14          11\nhis           11           6\nact            8           3\nhe             7           8\na              7           6\nas             6           6",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count_Obama</th>\n      <th>count_Bush</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>the</th>\n      <td>40</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>in</th>\n      <td>30</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>21</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>of</th>\n      <td>18</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>to</th>\n      <td>14</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>his</th>\n      <td>11</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>act</th>\n      <td>8</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>he</th>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>a</th>\n      <td>7</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>as</th>\n      <td>6</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bush_words = top_words('George W. Bush')\n",
    "# Modify the code to avoid error.\n",
    "obama_words.join(bush_words, lsuffix='_Obama', rsuffix='_Bush', how='inner').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note.** Even though common words are swamping out important subtle differences, commonalities in rarer political words still matter on the margin. This is why politicians are being listed in the query result instead of musicians, for example. In the next subsection, we will introduce a different metric that will place greater emphasis on those rarer words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Among the words that appear in both Barack Obama and Francisco Barrio, take the 15 that appear most frequently in Obama. How many of the articles in the Wikipedia dataset contain all of those 15 words? Which are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:07:27.812148Z",
     "start_time": "2023-12-31T11:07:27.452213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles that contain all of those 15 words: 30\n"
     ]
    }
   ],
   "source": [
    "# It might be helpful to use:\n",
    "word_to_ind={v: i for i, v in enumerate(vectorizer.get_feature_names_out())} # a dictionary with words as keys and indices as values\n",
    "\n",
    "# Your code goes here\n",
    "most_frequent_obama_words = common_words.sort_values(by='count_Obama', ascending=False).head(15).index\n",
    "most_frequent_obama_words_indexes = [word_to_ind[word] for word in most_frequent_obama_words]\n",
    "# articles.sum()\n",
    "articles_index = [x for x, y in enumerate(np.all(WCmatrix[:, most_frequent_obama_words_indexes].todense()>0, axis=1).tolist()) if y[0] == True]\n",
    "\n",
    "articles = wiki.loc[articles_index]\n",
    "print(f'Number of articles that contain all of those 15 words: {articles.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:07:27.813053Z",
     "start_time": "2023-12-31T11:07:27.649436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1177                            Donald Fowler\n1413                            Chris Redfern\n3400                            James Bilbray\n4004                              Paul Kagame\n4874                            Bernard Kenny\n6617                               Paul Sarlo\n11316                Gy%C3%B6rgy Sur%C3%A1nyi\n12371                         Morley Winograd\n12743                 David Ibarra Mu%C3%B1oz\n13229                        Francisco Barrio\n16095    Charles Taylor (Liberian politician)\n24417                           Jesse Ventura\n24478                               Joe Biden\n28447                          George W. Bush\n29505                        Arturo Vallarino\n33744                        John O. Agwunobi\n35541                            Jimmy Carter\n35817                            Barack Obama\n36452                            Bill Clinton\n38081                          John Garamendi\n39489                          Helmut Anheier\n40229                            Edward Rowny\n42934      Henry Sanders (Alabama politician)\n48253                 Saber Hossain Chowdhury\n50868                           Russell Trood\n52229                     Robert Lewis Morgan\n53102                             Ewart Brown\n54765                 Chuck Wolfe (executive)\n55495                      Lokman Singh Karki\n56172                               Hu Jintao\nName: name, dtype: object"
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# articles that contain all of those 15 words\n",
    "articles['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex. 3: TF-IDF to the rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the perceived commonalities between Obama and Barrio were due to occurrences of extremely frequent words, such as \"the\", \"and\", and \"his\". So nearest neighbors is recommending plausible results sometimes for the wrong reasons.\n",
    "\n",
    "To retrieve articles that are more relevant, we should focus more on rare words that don't happen in every article. **TF-IDF** (term frequency–inverse document frequency) is a feature representation that penalizes words that are too common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Repeat the search for the 10 nearest neighbors of Barack Obama with Euclidean distance of TF-IDF. This time do not limit to only 10000 most frequent words, but take all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:09:05.546559Z",
     "start_time": "2023-12-31T11:08:51.205397Z"
    }
   },
   "outputs": [],
   "source": [
    "# We could use:\n",
    "    # from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# but since we already know how to compute CountVectorizer, let's use:\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "vectorizer.fit_transform(wiki['text'])\n",
    "\n",
    "WCmatrix=vectorizer.transform(wiki['text'])\n",
    "\n",
    "tfidf=TfidfTransformer(smooth_idf=False, norm=None)\n",
    "TFIDFmatrix = tfidf.fit_transform(WCmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:09:10.718953Z",
     "start_time": "2023-12-31T11:09:10.618555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                       name  BO-eucl-TF-IDF\n42487                        Lee Shin Cheng        0.000000\n7653              Rodney Marsh (footballer)      164.067931\n31470                          Adam Haslett      164.517226\n6831                         Maurice Strong      164.540876\n15907                           Alan Duncan      164.722324\n44989                     Liu Yong (writer)      164.832381\n18542                            Tommy Haas      164.891675\n35225                        Adrian Burrows      165.278757\n11629                   Gregory Alan Isakov      165.446861\n29574  Billy Wright (footballer, born 1958)      165.506043",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>BO-eucl-TF-IDF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>42487</th>\n      <td>Lee Shin Cheng</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>7653</th>\n      <td>Rodney Marsh (footballer)</td>\n      <td>164.067931</td>\n    </tr>\n    <tr>\n      <th>31470</th>\n      <td>Adam Haslett</td>\n      <td>164.517226</td>\n    </tr>\n    <tr>\n      <th>6831</th>\n      <td>Maurice Strong</td>\n      <td>164.540876</td>\n    </tr>\n    <tr>\n      <th>15907</th>\n      <td>Alan Duncan</td>\n      <td>164.722324</td>\n    </tr>\n    <tr>\n      <th>44989</th>\n      <td>Liu Yong (writer)</td>\n      <td>164.832381</td>\n    </tr>\n    <tr>\n      <th>18542</th>\n      <td>Tommy Haas</td>\n      <td>164.891675</td>\n    </tr>\n    <tr>\n      <th>35225</th>\n      <td>Adrian Burrows</td>\n      <td>165.278757</td>\n    </tr>\n    <tr>\n      <th>11629</th>\n      <td>Gregory Alan Isakov</td>\n      <td>165.446861</td>\n    </tr>\n    <tr>\n      <th>29574</th>\n      <td>Billy Wright (footballer, born 1958)</td>\n      <td>165.506043</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now recompute the distances as before but for TF-IDF\n",
    "\n",
    "dist = pairwise_distances(TFIDFmatrix, Y=TFIDFmatrix[wiki.loc[wiki['name'] == 'Barack Obama'].index[0]])\n",
    "# add the distances as a column in the wiki dataframe\n",
    "wiki['BO-eucl-TF-IDF'] = dist\n",
    "wiki.sort_values('BO-eucl-TF-IDF').head(n=10)[['name', 'BO-eucl-TF-IDF']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine whether this list makes sense.\n",
    "* With a notable exception of Nathan Cullen, the other 8 are all American politicians who are contemporaries of Barack Obama.\n",
    "* Phil Schiliro, Jesse Lee, Samantha Power, Eric Stern, Eric Holder worked for Obama.\n",
    "\n",
    "Clearly, the results are more plausible with the use of TF-IDF. Let's take a look at the word vector for Obama and Schilirio's pages. Notice that TF-IDF representation assigns a weight to each word. This weight captures relative importance of that word in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Sort the words in Obama's article by their TF-IDF weights; do the same for Schiliro's article as well.\n",
    "Using the **join** operation we learned earlier, compute the common words shared by Obama's and Schiliro's articles.\n",
    "Sort the common words by their TF-IDF weights in Obama's document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-12-31T11:07:43.222865Z"
    }
   },
   "outputs": [],
   "source": [
    "def top_words_tf_idf(name):\n",
    "    \"\"\"\n",
    "    Get a table of the largest tf-idf words in the given person's wikipedia page.\n",
    "    \"\"\"\n",
    "    # Your code goes here\n",
    "    \n",
    "    return df.sort_values(by='tf-idf',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-31T11:07:43.224658Z"
    }
   },
   "outputs": [],
   "source": [
    "obama_tf_idf = top_words_tf_idf('Barack Obama')\n",
    "schiliro_tf_idf = top_words_tf_idf('Phil Schiliro')\n",
    "common_words = # Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Among the words that appear in both Barack Obama and Phil Schiliro, take the 15 that have largest weights in Obama. How many of the articles in the Wikipedia dataset contain all of those 15 words? Which are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T11:07:43.250495Z",
     "start_time": "2023-12-31T11:07:43.226228Z"
    }
   },
   "outputs": [],
   "source": [
    "# It might be helpful to use:\n",
    "word_to_ind={v: i for i, v in enumerate(vectorizer.get_feature_names())} # a dictionary with words as keys and indices as values\n",
    "\n",
    "# Your code goes here\n",
    "\n",
    "articles.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-31T11:07:43.227907Z"
    }
   },
   "outputs": [],
   "source": [
    "wiki[articles]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the huge difference in this calculation using TF-IDF scores instead  of raw word counts. We've eliminated noise arising from extremely common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex. 4: Choosing metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Compute the Euclidean distance between TF-IDF features of Obama and Biden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-31T11:07:43.229211Z"
    }
   },
   "outputs": [],
   "source": [
    "dist = # Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance is larger than the distances we found for the 10 nearest neighbors, which we repeat here for readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-31T11:07:43.230670Z"
    }
   },
   "outputs": [],
   "source": [
    "wiki.sort_values(by='BO-eucl-TF-IDF',ascending=True)[['name','BO-eucl-TF-IDF']][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But one may wonder, is Biden's article that different from Obama's, more so than, say, Schiliro's? It turns out that, when we compute nearest neighbors using the Euclidean distances, we unwittingly favor short articles over long ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Let us compute the length of each Wikipedia document, and examine the document lengths for the 100 nearest neighbors to Obama's page. To compute text length use the same splitting rules you used in `vectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-31T11:07:43.231817Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = # Your code goes here\n",
    "\n",
    "def compute_length(row):\n",
    "# Here we could use simply:\n",
    "#     return len(row['text'].split(' '))\n",
    "    return len(tokenizer(row['text']))\n",
    "\n",
    "wiki['length'] = # Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-31T11:07:43.233515Z"
    }
   },
   "outputs": [],
   "source": [
    "nearest_neighbors_euclidean = # Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** To see how these document lengths compare to the lengths of other documents in the corpus, make a histogram of the document lengths of Obama's 100 nearest neighbors and compare to a histogram of document lengths for all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-31T11:07:43.236151Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10.5,4.5))\n",
    "plt.hist(# Your code goes here\n",
    "        )\n",
    "plt.axvline(# Your code goes here\n",
    "        )\n",
    "\n",
    "# Your code goes here\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative to the rest of Wikipedia, nearest neighbors of Obama are overwhemingly short, most of them being shorter than 300 words. The bias towards short articles is not appropriate in this application as there is really no reason to  favor short articles over long articles (they are all Wikipedia articles, after all). Many of the Wikipedia articles are 300 words or more, and both Obama and Biden are over 300 words long.\n",
    "\n",
    "**Note**: For the interest of computation time, the dataset given here contains _excerpts_ of the articles rather than full text. For instance, the actual Wikipedia article about Obama is around 25000 words. Do not be surprised by the low numbers shown in the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Both word-count features and TF-IDF are proportional to word frequencies. While TF-IDF penalizes very common words, longer articles tend to have longer TF-IDF vectors simply because they have more words in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove this bias, we turn to **cosine distances**:\n",
    "$$\n",
    "d(\\mathbf{x},\\mathbf{y}) = 1 - \\frac{\\mathbf{x}^T\\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|}\n",
    "$$\n",
    "Cosine distances let us compare word distributions of two articles of varying lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Train a new nearest neighbor model, this time with cosine distances.  Then repeat the search for Obama's 100 nearest neighbors and make a plot to better visualize the effect of having used cosine distance in place of Euclidean on our TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-31T11:07:43.237735Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "nearest_neighbors_cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a glance at the above table, things look better.  For example, we now see Joe Biden as Barack Obama's nearest neighbor!  We also see Hillary Clinton on the list.  This list looks even more plausible as nearest neighbors of Barack Obama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-31T11:07:43.238717Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10.5,4.5))\n",
    "plt.hist(# Your code goes here\n",
    "        )\n",
    "plt.axvline(# Your code goes here\n",
    "        )\n",
    "\n",
    "# Your code goes here\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the 100 nearest neighbors using cosine distance provide a sampling across the range of document lengths, rather than just short articles like Euclidean distance provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moral of the story**: In deciding the features and distance measures, check if they produce results that make sense for your particular application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex. 5: Problem with cosine distances: tweets vs. long articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happily ever after? Not so fast. Cosine distances ignore all document lengths, which may be great in certain situations but not in others. For instance, consider the following (admittedly contrived) example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "+--------------------------------------------------------+\n",
    "|                                             +--------+ |\n",
    "|  One that shall not be named                | Follow | |\n",
    "|  @username                                  +--------+ |\n",
    "|                                                        |\n",
    "|  Democratic governments control law in response to     |\n",
    "|  popular act.                                          |\n",
    "|                                                        |\n",
    "|  8:05 AM - 16 May 2016                                 |\n",
    "|                                                        |\n",
    "|  Reply   Retweet (1,332)   Like (300)                  |\n",
    "|                                                        |\n",
    "+--------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Transform the tweet into TF-IDF features, using the fit to the Wikipedia dataset. (That is, let's treat this tweet as an article in our Wikipedia dataset and see what happens.) How similar is this tweet to Barack Obama's Wikipedia article? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-31T11:07:43.239600Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text': ['democratic governments control law in response to popular act']})\n",
    "\n",
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this tweet's TF-IDF vectors  to Barack Obama's Wikipedia entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-31T11:07:43.240689Z"
    }
   },
   "outputs": [],
   "source": [
    "obama_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Now, compute the cosine distance between the Barack Obama article and this tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-31T11:07:43.241706Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances # for one pair of samples we can just use this function\n",
    "\n",
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this distance to the distance between the Barack Obama article and all of its Wikipedia nearest neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-31T11:07:43.242542Z"
    }
   },
   "outputs": [],
   "source": [
    "nearest_neighbors_cosine[0:23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cosine distances, the tweet is \"nearer\" to Barack Obama than most people! If someone is reading the Barack Obama Wikipedia page, would you want to recommend they read this tweet?\n",
    "In practice, it is common to enforce maximum or minimum document lengths. After all, when someone is reading a long article from _The Atlantic_, you wouldn't recommend him/her a tweet."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
